{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import xml.etree.ElementTree as ET\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Util functions\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "pos_tags = ['.','ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n",
    "n_tags = len(pos_tags)\n",
    "le = LabelEncoder().fit(pos_tags)\n",
    "\n",
    "## load_embeddings ##\n",
    "# Load previously generated embeddings to build a dictionary label->embedding\n",
    "# -input\n",
    "# path_embeddings: path to the .txt file with the embeddings. Format is label<-space->embedding.\n",
    "# -output\n",
    "# embedding_dict: dictionary label -> embedding\n",
    "# hidden_size: size of the embeddings\n",
    "def load_embeddings(path_embeddings):\n",
    "\n",
    "\tembedding_dict = dict()      \n",
    "\n",
    "\twith open(path_embeddings,'r') as f:\n",
    "\n",
    "\t\tfor embedding_pair in f.readlines():\n",
    "\n",
    "\t\t\tlabel, embedding = embedding_pair.split(' ',1)\n",
    "\t\t\tembedding = np.array(embedding.split(),dtype=float)\n",
    "\t\t\tembedding_dict[label.strip()] = embedding\n",
    "\n",
    "\thidden_size = len(embedding)\n",
    "\n",
    "\treturn embedding_dict,hidden_size\n",
    "\n",
    "## compute_output_space ##\n",
    "# computes the necessary data structure for the output space.\n",
    "# -input\n",
    "# xml_file: xml file in input containing the sentences\n",
    "# ground_truth: txt file containing the babelsids for each ambiguous lemma\n",
    "# -output\n",
    "# senses_dict : dictionary sense -> code\n",
    "# truesenses_dict: dictionary instance_id -> sense\n",
    "# wf_lemmas_dict: dictionary lemma (of unambiguos words) -> code\n",
    "# senses_per_lemma_dict: dictionary lemma (of ambiguos word) -> list of sense codes\n",
    "# out_size: size of the output space (+1 for padding)\n",
    "def compute_output_space(xml_file,ground_truth):\n",
    "    \n",
    "\ttree = ET.parse(xml_file)\n",
    "\troot = tree.getroot()\n",
    "\n",
    "\tsenses_dict = dict()                        #dictionary: sense -> code\n",
    "\ttruesenses_dict = dict()                    #dictionary: instance_id -> sense\n",
    "\twf_lemmas_dict = dict()                     #dictionary: lemma (of unambiguos words) -> code\n",
    "\tsenses_per_lemma_dict = dict()\t\t\t\t#dictionary: lemma (of ambiguos word) -> list of sense codes\n",
    "\n",
    "\t#code 0 is used for padding\n",
    "\tcode = 1\n",
    "\t#Senses\n",
    "\twith open(ground_truth,'r') as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tinstance_id , sense = line.split()\n",
    "\n",
    "\t\t\ttruesenses_dict[instance_id] = sense\n",
    "\t\t\tif sense not in senses_dict:\n",
    "\t\t\t\tsenses_dict[sense] = code\n",
    "\t\t\t\tcode += 1\n",
    "\n",
    "\t#Lemmas\n",
    "\tfor text in root.getchildren():\n",
    "\t\tfor sentence in text:\n",
    "\t\t\tfor word in sentence:\n",
    "\t\t\t\tlemma = word.attrib['lemma']\n",
    "                \n",
    "                #Unambiguous lemma\n",
    "\t\t\t\tif word.tag == 'wf' and lemma not in wf_lemmas_dict: #add the new lemma to the dictionary\t\n",
    "\t\t\t\t\twf_lemmas_dict[lemma] = code\n",
    "\t\t\t\t\tcode += 1\n",
    "                    \n",
    "                #Ambiguous lemma\n",
    "\t\t\t\telif word.tag == 'instance':\n",
    "                    # Take a look at the sense\n",
    "\t\t\t\t\tsense = senses_dict[truesenses_dict[word.attrib['id']]]\n",
    "                    # First time we found a sense for the lemma\n",
    "\t\t\t\t\tif lemma not in senses_per_lemma_dict:\n",
    "\t\t\t\t\t\tsenses_per_lemma_dict[lemma] = []\n",
    "                    # The new sense is added to the possible senses of the lemma\n",
    "\t\t\t\t\tif sense not in senses_per_lemma_dict[lemma]:\n",
    "\t\t\t\t\t\tsenses_per_lemma_dict[lemma].append(sense)\n",
    "\n",
    "    \n",
    "\tout_size = code\n",
    "\n",
    "\treturn senses_dict,truesenses_dict,wf_lemmas_dict,senses_per_lemma_dict,out_size\n",
    "\n",
    "## generate_word_vector ##\n",
    "# returns the word vector using the lemma \n",
    "# input\n",
    "# lemma: lemma of the word\n",
    "# pos: pos of the word\n",
    "# embedding_dict: dictionary label -> embedding\n",
    "# output\n",
    "# word_vector: the word embedding for the lemma + pos value\n",
    "def generate_word_vector(lemma,pos,embedding_dict):\n",
    "    \n",
    "\tpos_value = le.transform([pos])[0]\n",
    "    \n",
    "\tif lemma in embedding_dict:\n",
    "\t\twv = embedding_dict[lemma]\n",
    "\telse:\n",
    "\t\twv = embedding_dict['unk']\n",
    "    \n",
    "\treturn np.append(wv,pos_value)\n",
    "\n",
    "## generate_label ##\n",
    "# Returns a pair of label for the word. The first label depends on the lemma. If the lemma in unambiguous then\n",
    "# the first label is the lemma, otherwise it is the correct sense. The second label is the pos tag.\n",
    "# -input\n",
    "# word: child node of some sentence node\n",
    "# senses_dict : dictionary sense -> code\n",
    "# truesenses_dict: dictionary instance_id -> sense\n",
    "# wf_lemmas_dict: dictionary lemma -> code\n",
    "# -output\n",
    "# label: code of the label + code for the pos\n",
    "\n",
    "def generate_label(word,senses_dict,truesenses_dict,wf_lemmas_dict):\n",
    "    \n",
    "\tpos = word.attrib['pos']\n",
    "\tcode_pos = le.transform([pos])[0]\n",
    "\n",
    "    #Unambiguous\n",
    "\tif word.tag == 'wf':\n",
    "\t\tlemma = word.attrib['lemma']\n",
    "\t\tif lemma in wf_lemmas_dict:\n",
    "\t\t\tcode_label = wf_lemmas_dict[lemma]\n",
    "\t\telse:\n",
    "\t\t\tcode_label = 0            #Predict 0 for the unk lemma (don't care)\n",
    "            \n",
    "    #Ambiguous\n",
    "\telif word.tag == 'instance':\n",
    "\t\tsense = truesenses_dict[word.attrib['id']]\n",
    "\t\tif sense in senses_dict:\n",
    "\t\t\tcode_label = senses_dict[sense]\n",
    "\t\telse:\n",
    "\t\t\tcode_label = -1          #The sense is unknown by the system. Taken into account by the recall.\n",
    "        \n",
    "\treturn np.append(code_label,code_pos)\n",
    "\n",
    "## generate_code_choices ##\n",
    "# Returns all possible choices for a lemma (limiting the output space). If the word is a unambiguous lemma then \n",
    "# we don't care about the choice. If the lemma is ambiguous then we return all possible senses. If the system \n",
    "# doesn't recognize the sense of the lemma it returns -1 which will be taken into account by the recall.\n",
    "# -input\n",
    "# word: child node of some sentence node\n",
    "# senses_per_lemma_dict : dictionary lemma (ambiguous) -> list of possible codes\n",
    "# -output\n",
    "# val: 0,-1 or a list\n",
    "def generate_code_choices(child,senses_per_lemma_dict):\n",
    "    #Unambiguous lemma\n",
    "\tif child.tag == 'wf':\n",
    "\t\tval = 0\n",
    "\telse:\n",
    "    #Ambiguous lemma\n",
    "\t\tif child.attrib['lemma'] in senses_per_lemma_dict:   #Else if it has known senses, append the list of code\n",
    "\t\t\tval = senses_per_lemma_dict[child.attrib['lemma']]\n",
    "\t\telse:\n",
    "\t\t\tval = -1                                         #Otherwise cannot disambiguate\n",
    "            \n",
    "\treturn val\n",
    "    \n",
    "## pad_split ##\n",
    "# Given in input a list of lists, pad_split pads each list with pad_token to a multiple of window_size\n",
    "# to build a list of lists of fixed sizes.\n",
    "# -input\n",
    "# in_lists: list of lists to be padded\n",
    "# pad_token: token used to pad the internal lists\n",
    "# window_size: fixed size of the window\n",
    "# window_overlap: overlap between two consecutive windows\n",
    "# -output\n",
    "# padded_lists: list of lists of padded & split lists\n",
    "# original_lengths: the original lenghts of the lists\n",
    "def pad_split(in_lists,pad_token,window_size,window_overlap):\n",
    "\n",
    "\n",
    "\tpadded_lists = []\n",
    "\toriginal_lengths = []\n",
    "\n",
    "\tfor in_list in in_lists:\n",
    "\n",
    "\t\t#Splitting\n",
    "\t\tlist_parts = [in_list[i:i + window_size] for i in range(0, len(in_list), window_size - window_overlap)]\n",
    "\t\tlen_parts = [len(piece) for piece in list_parts]\n",
    "\n",
    "\t\t#Padding could be required just for the last element\n",
    "\t\tdiff = window_size - len_parts[-1]\n",
    "\n",
    "\t\tfor i in range(diff):\n",
    "\t\t\tlist_parts[-1].append(pad_token)\n",
    "\n",
    "\t\tpadded_lists += list_parts\n",
    "\t\toriginal_lengths += len_parts\n",
    "\n",
    "\n",
    "\treturn padded_lists, original_lengths\n",
    "\n",
    "## xml_parser ##\n",
    "# parse a xml file given the text nodes (padding applied). It uses all previously generated data structures\n",
    "# to produce a list of (fixed size) sentences, labels and lens used to feed the netwok\n",
    "# -input\n",
    "# texts: list of text nodes in a xml file\n",
    "# window_size: fixed size of the window over the sentence\n",
    "# window_overlap: overlap between windows\n",
    "# embedding_dict: dictionary label -> embedding\n",
    "# senses_dict : dictionary sense -> code\n",
    "# truesenses_dict: dictionary instance_id -> sense\n",
    "# wf_lemmas_dict: dictionary lemma (of unambiguos words) -> code\n",
    "# senses_per_lemma_dict: dictionary lemma (of ambiguos word) -> list of sense codes\n",
    "# -output\n",
    "# sentences: parsed sentences, each word is replaced by its feature vector. Each sentence has a fixed size.\n",
    "# labels: vector of labels, same structure of sentences\n",
    "# lens: list of the original lengths of the sentences\n",
    "# code_choices: list of possible codes for each word in the sentence\n",
    "def xml_parser(texts,window_size,window_overlap,embedding_dict,senses_dict,truesenses_dict,wf_lemmas_dict,senses_per_lemma_dict):\n",
    "    \n",
    "\tsentences = []                                   \n",
    "\tlabels = []                                      \n",
    "\tlens = []                                        \n",
    "\tcode_choices = []                                 \n",
    "    \n",
    "\tfor text in texts:\n",
    "\t\tfor sentence in text:\n",
    "            \n",
    "\t\t\tsentence_words = []\n",
    "\t\t\tlabels_words = []\n",
    "\t\t\tcode_choices_words = []\n",
    "        \n",
    "\t\t\tfor child in sentence.getchildren():\n",
    "                \n",
    "\t\t\t\tsentence_words.append(generate_word_vector(child.attrib['lemma'],child.attrib['pos'],embedding_dict))\n",
    "\t\t\t\tlabels_words.append(generate_label(child,senses_dict,truesenses_dict,wf_lemmas_dict))\n",
    "                \n",
    "    \n",
    "\n",
    "\t\t\tsentences.append(sentence_words)\n",
    "\t\t\tlabels.append(labels_words)\n",
    "\t\t\tcode_choices.append(code_choices_words)\n",
    "\t\n",
    "\treturn sentences,labels,lens,code_choices\n",
    "\n",
    "\n",
    "## scores_senses ##\n",
    "# computes the precision,recall and the the f1 score taking into account only the senses\n",
    "# input\n",
    "# prediction_values : prediction list of lists of probabilities over the output space\n",
    "# true labels : true labels (int codes)\n",
    "# final_code: end of the code for senses\n",
    "# output\n",
    "# precision: correct_senses/total_sense\n",
    "# recall: correct_sense/predicted_senses\n",
    "# f1_score: f1 score\n",
    "def precision_senses(prediction_values,true_labels,mask_values,final_code):\n",
    "    \n",
    "\tcorrect_senses = 0\n",
    "\tdisambiguated_senses = 0\n",
    "\tskipped_senses = 0\n",
    "    \n",
    "\tfor snt_pre, snt_tru, snt_msk in zip(prediction_values,true_labels,mask_values):  #Cycling the batch\n",
    "\t\tfor vect_pre, val_tru, val_msk in zip(snt_pre,snt_tru,snt_msk):               #Cycling the sentence\n",
    "            \n",
    "\t\t\tif val_msk == 0: #Either wf_lemma or padding, don't care\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif val_msk == -1: #Avoid to predict the sense\n",
    "\t\t\t\tskipped_senses += 1\n",
    "\t\t\t\tcontinue\n",
    "                \n",
    "            #Ambiguous lemma, choose only from the senses related to it\n",
    "            \n",
    "\t\t\tdisambiguated_senses += 1\n",
    "            \n",
    "\t\t\tprediction_for_lemma = val_msk[np.argmax(vect_pre[val_msk])]\n",
    "\t\t\tif val_tru == prediction_for_lemma:\n",
    "\t\t\t\tcorrect_senses += 1\n",
    "                \n",
    "\tprecision = correct_senses/disambiguated_senses\n",
    "\trecall = correct_senses/(correct_senses+skipped_senses)\n",
    "\tf1_score = (2*precision*recall)/(precision + recall)\n",
    "\treturn precision,recall,f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "Given a word in a sentence, we transform it in a feature vector which includes the embedding vector and the POS tag. Thus, a sentence is a sequence of features vectors of the same size. The input space of our network will then be all the possibile features vector.\n",
    "\n",
    "The output space will be the union of the lemmas of unambiguous words and the possibile senses FOUND in the training data. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output space size is: 49219\n",
      "The features vectors have size: 301\n"
     ]
    }
   ],
   "source": [
    "#embedding_dict is a dictionary: word -> embedding\n",
    "#hidden_size is the size of the embeddings\n",
    "embedding_dict, hidden_size = load_embeddings('glove.6B.300d.txt')\n",
    "\n",
    "#the following function generates suitable data structures to represent the output space FROM the training data.\n",
    "#sense_dict is a dictionary: sense -> code\n",
    "#truesenses_dict is a dictionary: instance_id -> sense\n",
    "#wf_lemmas_dict is a dictionary: lemma -> code\n",
    "#senses_per_lemma_dict: lemma (of an ambiguous word) -> list of possible code senses for the lemma\n",
    "#out_size is the size of the output space\n",
    "senses_dict, truesenses_dict, wf_lemmas_dict,senses_per_lemma_dict, out_size = compute_output_space('semcor.data.xml','semcor.gold.key.bnids.txt')\n",
    "print('The output space size is: ' + str(out_size),flush=True)\n",
    "\n",
    "#The feature vector will be embedding vector (300) + POS tag (1)\n",
    "n_features = hidden_size + 1 \n",
    "print('The features vectors have size: ' + str(n_features))\n",
    "\n",
    "#Window size for the padding\n",
    "window_size = 30\n",
    "window_overlap = 5\n",
    "\n",
    "keep_prob = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "We load the training data with a xml_parser function which uses the previously generated data structures to get a list of sentences, labels and sentences' lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('semcor.data.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "tr_sentences,tr_labels,tr_lens,tr_amb_lemmas = xml_parser(root.getchildren(),window_size,window_overlap,embedding_dict,senses_dict,truesenses_dict,wf_lemmas_dict,senses_per_lemma_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data\n",
    "Being the instance_id in the validation data different from the ones found in training data, truesenese_dict needs to be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ALL.gold.key.bnids.txt','r') as f:\n",
    "    for line in f.readlines():\n",
    "        instance_id, sense = line.split()\n",
    "        truesenses_dict[instance_id] = sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the validation data but splitting by dataset. The first part just reads the names of dataset in the data. The second part loads the validation data creating dev_data which is organized as follow:\n",
    "\n",
    "dev_data = [D_1,D_2,..] where D_x is a dataset\n",
    "\n",
    "D_x = [sentences,labels,lens] for that dataset\n",
    "\n",
    "sentences,labels,lens are structed in the same way as before\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('ALL.data.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "dev_texts = []                                      #list of lists: text split by datasets\n",
    "datasets_names = []                                 #list: names of the found datasets\n",
    "\n",
    "dev_data = []      \n",
    "\n",
    "#First part\n",
    "for text in root.getchildren():\n",
    "    dataset_name = text.attrib['id'].split('.')[0]\n",
    "    \n",
    "    if dataset_name not in datasets_names:  # add the new dataset\n",
    "        datasets_names.append(dataset_name)\n",
    "        dev_texts.append([])\n",
    "        \n",
    "    dev_texts[datasets_names.index(dataset_name)].append(text) #assign the (text) node to the right dataset\n",
    "\n",
    "#Second part\n",
    "for dataset in dev_texts:\n",
    "    \n",
    "    dv_sentences,dv_labels,dv_lens,dv_amb = xml_parser(dataset,window_size,window_overlap,embedding_dict,senses_dict,truesenses_dict,wf_lemmas_dict,senses_per_lemma_dict)\n",
    "    \n",
    "    dev_data.append([dv_sentences,dv_labels,dv_lens,dv_amb])\n",
    "\n",
    "del dev_texts,dv_sentences,dv_labels,dv_lens, dv_amb,root, tree #removing useless data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network\n",
    "The network is structured as following:\n",
    "\n",
    "Sentences, labels and lengths are read trought placeholder.There is one layer of BiLSTM. Lastly there are two output layer, one used for the label prediction and the other for pos tag prediction.\n",
    "\n",
    "    inputs (sentences) has shape (batch_size,window_size,n_features)\n",
    "    targets (labels) has shape (batch_size,window_size,2) (2 = word_label + pos_label)\n",
    "    seq_lens (lens) has shape (batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, shape=[None,window_size,n_features])\n",
    "targets = tf.placeholder(tf.int32, shape=[None,None,2])\n",
    "seq_lens = tf.placeholder(tf.int32, shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_units = 500\n",
    "\n",
    "cell_fw = tf.contrib.rnn.LSTMCell(lstm_units)\n",
    "cell_bw = tf.contrib.rnn.LSTMCell(lstm_units)\n",
    "\n",
    "#Dropout\n",
    "cell_fw = tf.nn.rnn_cell.DropoutWrapper(cell_fw, output_keep_prob=keep_prob)\n",
    "cell_bw = tf.nn.rnn_cell.DropoutWrapper(cell_bw, output_keep_prob=keep_prob)\n",
    "\n",
    "outputs,_ = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs,sequence_length=seq_lens, dtype=tf.float32)\n",
    "\n",
    "#Concatenation of the two hidden states\n",
    "full_output = tf.concat([outputs[0],outputs[1]],axis=-1)\n",
    "#full_output has shape (batch_size,window_size,lstm_units*2)\n",
    "\n",
    "#Generating a mask to ignore padding\n",
    "mask = tf.sequence_mask(seq_lens)\n",
    "#mask has shape (batch_size,window_size)\n",
    "\n",
    "#Mask repated in time for the attention layer\n",
    "mask_repeated = tf.reshape(tf.tile(mask,[1,window_size]),(-1,window_size,window_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithContext:\n",
    "    def __init__(self,input_shape, **kwargs):\n",
    "        super(AttentionWithContext,self).__init__(**kwargs)\n",
    "        self.build(input_shape)\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        dense = tf.keras.layers.Dense(input_shape[1],activation=tf.nn.tanh,use_bias=True)\n",
    "        self.td = tf.keras.layers.TimeDistributed(dense)\n",
    "        \n",
    "    def __call__(self,inputs,mask):\n",
    "        focused_a = self.td(inputs)\n",
    "        focused_a = tf.exp(focused_a)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask, tf.float32)\n",
    "            foucsed_a = mask * focused_a\n",
    "            \n",
    "        focused_a /= tf.cast(tf.reduce_sum(focused_a,axis=1,keepdims=True) + tf.keras.backend.epsilon(),tf.float32)\n",
    "        \n",
    "        focused_features = tf.keras.backend.batch_dot(focused_a,inputs)\n",
    "        return [focused_features,focused_a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_output, _ = AttentionWithContext(full_output.get_shape().as_list())(full_output,mask_repeated)\n",
    "\n",
    "output_concat = tf.concat([full_output,att_output],-1)\n",
    "\n",
    "#Size of the last dimension after the concatenation\n",
    "aug_features = output_concat.get_shape()[-1]\n",
    "\n",
    "#Flattening\n",
    "output_flat = tf.reshape(output_concat,[-1,aug_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_w = 1024\n",
    "\n",
    "W_fc = tf.get_variable('W_fc',shape=[aug_features,fc_w],dtype=tf.float32)\n",
    "b_fc = tf.get_variable('b_fc',shape=[fc_w],dtype=tf.float32) \n",
    "\n",
    "h_fc = tf.nn.relu(tf.matmul(output_flat,W_fc) + b_fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing the targets placeholder to extract the label for the word and the pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = tf.squeeze(tf.slice(targets,(0,0,0),(tf.shape(targets)[0],tf.shape(targets)[1],1)),-1)\n",
    "target_pos = tf.squeeze(tf.slice(targets,(0,0,1),(tf.shape(targets)[0],tf.shape(targets)[1],1)),-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss the label for the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_label = tf.get_variable('W_label',shape=[fc_w,out_size],dtype=tf.float32)\n",
    "b_label = tf.get_variable('b_label',shape=[out_size],dtype=tf.float32) \n",
    "\n",
    "logits_label = tf.matmul(h_fc,W_label) + b_label\n",
    "logits_label = tf.reshape(logits_label,[-1,window_size,out_size])\n",
    "\n",
    "losses_label = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_label,labels=target_label) #UNSCALED LOGITS!\n",
    "#losses_label has shape (batch_size,window_size). Computes the loss for each word\n",
    "\n",
    "#Ignore the losses from padding\n",
    "losses_label = tf.boolean_mask(losses_label,mask)\n",
    "#losses_label has shape (n) where n is the sum of the lens\n",
    "\n",
    "#Summing the losses\n",
    "loss_label = tf.reduce_sum(losses_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss the POS for the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_pos = tf.get_variable('W_pos',shape=[aug_features,n_tags],dtype=tf.float32)\n",
    "b_pos = tf.get_variable('b_pos',shape=[n_tags],dtype=tf.float32) \n",
    "\n",
    "logits_pos = tf.matmul(output_flat,W_pos) + b_pos\n",
    "logits_pos = tf.reshape(logits_pos,[-1,window_size,n_tags])\n",
    "\n",
    "losses_pos = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_pos,labels=target_pos)\n",
    "losses_pos = tf.boolean_mask(losses_pos,mask)\n",
    "loss_pos = tf.reduce_sum(losses_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "total_loss = loss_pos + loss_label\n",
    "\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.5).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_label_values = tf.nn.softmax(logits_label)\n",
    "preds_label = tf.argmax(preds_label_values,axis=-1)\n",
    "\n",
    "#Ignorining the predictions for the padding\n",
    "preds_label = tf.boolean_mask(preds_label,mask)\n",
    "true_labels = tf.boolean_mask(target_label,mask)\n",
    "\n",
    "preds_pos = tf.nn.softmax(logits_pos)\n",
    "preds_pos = tf.argmax(preds_pos,axis=-1)\n",
    "\n",
    "preds_pos = tf.boolean_mask(preds_pos,mask)\n",
    "true_pos = tf.boolean_mask(target_pos,mask)\n",
    "\n",
    "#Accuracy on POS learning\n",
    "correct_pos = tf.cast(tf.equal(tf.cast(preds_pos,tf.int32), true_pos),tf.float32)\n",
    "accuracy_pos = tf.reduce_mean(correct_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('loss_label',loss_label)\n",
    "tf.summary.scalar('loss_pos',loss_pos)\n",
    "tf.summary.scalar('total_loss',total_loss)\n",
    "\n",
    "tf.summary.scalar('accuracy_pos',accuracy_pos)\n",
    "\n",
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('./summary', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key W_fc not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/karapost/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/karapost/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/karapost/.local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/karapost/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/karapost/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/karapost/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-32-97fb25fb742e>\", line 1, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1293, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1302, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1339, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 796, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 449, in _AddRestoreOps\n    restore_sequentially)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 847, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1030, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key W_fc not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key W_fc not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-97fb25fb742e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/model.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./model/model.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Previous model restored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1754\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1755\u001b[0;31m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1756\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1757\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key W_fc not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/karapost/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/karapost/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/karapost/.local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/karapost/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/karapost/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/karapost/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/karapost/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-32-97fb25fb742e>\", line 1, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1293, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1302, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1339, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 796, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 449, in _AddRestoreOps\n    restore_sequentially)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 847, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1030, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key W_fc not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "if not os.path.exists(\"model\"):\n",
    "    os.makedirs(\"model\")\n",
    "    \n",
    "if tf.train.checkpoint_exists('./model/model.ckpt'):\n",
    "    saver.restore(sess, './model/model.ckpt')\n",
    "    print(\"Previous model restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "\n",
    "batch_size = 32\n",
    "batch_index = 0\n",
    "num_batches_per_epoch = ceil(len(tr_lens)/batch_size)\n",
    "\n",
    "n_iterations = num_batches_per_epoch * epochs\n",
    "\n",
    "for ite in range(n_iterations):\n",
    "    \n",
    "    if ite % 10 == 0:\n",
    "        print('Iteration: ' + str(ite) )\n",
    "        \n",
    "    #Batch\n",
    "    batch_input = tr_sentences[batch_index*batch_size:(batch_index+1)*batch_size]\n",
    "    batch_label = tr_labels[batch_index*batch_size:(batch_index+1)*batch_size]\n",
    "    batch_lens = tr_lens[batch_index*batch_size:(batch_index+1)*batch_size]\n",
    "    batch_dv = tr_amb_lemmas[batch_index*batch_size:(batch_index+1)*batch_size]\n",
    "    \n",
    "    batch_index = (batch_index + 1 ) % num_batches_per_epoch\n",
    "    \n",
    "    if batch_index % 50 == 0:\n",
    "        saver.save(sess, './model/model.ckpt')\n",
    "        print('Development set scores:')\n",
    "        for dataset,name in zip(dev_data,datasets_names):\n",
    "            predicted_values,target_values, acc = sess.run([preds_label_values,target_label,accuracy_pos],feed_dict={ inputs : dataset[0],targets : dataset[1], seq_lens : dataset[2]})\n",
    "            precision,recall,f1_score = precision_senses(predicted_values,target_values,dataset[3],len(senses_dict))\n",
    "            print('{} : precision on senses {} recall on senses {} f1_score on senses {} accuracy on pos {}'.format(name,precision,recall,f1_score,acc),flush=True)\n",
    "\n",
    "    _,predicted_values,target_values,acc = sess.run([optimizer,preds_label_values,target_label,accuracy_pos],feed_dict={ inputs : batch_input, targets : batch_label, seq_lens : batch_lens})\n",
    "    precision,recall,f1_score = precision_senses(predicted_values,target_values,batch_dv,len(senses_dict))\n",
    "    print('Training: precision on senses {} f1_score on senses {} accuracy on pos {}'.format(precision,f1_score,acc),flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = []\n",
    "\n",
    "with open('test_data.txt','r') as f:\n",
    "    for line_sentence in f.readlines():\n",
    "        words = line_sentence.split()\n",
    "        test_sentence = []\n",
    "        for word in words:\n",
    "            word_parts = word.split('|')\n",
    "            \n",
    "            word_vector = generate_word_vector_pos(word_parts[1],word_parts[2],embedding_dict)\n",
    "            test_sentence.append(word_vector)\n",
    "        test_sentences.append(test_sentence)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
